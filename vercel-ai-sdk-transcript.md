what up Wizards I've got a treat for you today I've been building a lot of stuff with typescript and AI recently I've been building agents workflows custom scripts and clis to help me work faster and smarter and vel's aisk has been an amazing tool in my toolkit it's a library that solves a lot of the pain points when you're interacting with llms and in this massive video I'm going to give you a full tutorial on how to get to grips with it we're going to cover prompting streaming text structured outputs handling images and files tool calling and building agents is this a sponsored video no you don't need to pay or sell any money to use the aisk you don't need a cloud platform you don't even need to use nextjs you can use the AISD K anywhere JavaScript runs and that's why I'm so excited about it it's so versatile so powerful and it's just the right level of abstraction for almost any AI task this video is split into many many different parts each part teaches you one or two things about the AIS SDK if you want access to the code I strongly recommend you check out the host version on AI hero instead it's got all of these beautiful code samples as well as links to a GitHub repo where you can run this all locally it also splits up all the videos into individual URLs so that you can reference them easily so if you want that go to the description below and head there now but if you want the full YouTube experience let's freaking go there's a pretty common problem when you're building AI powered apps let's say that you're using open AI as your llm provider you build all of this code here just to to talk to open ai's API and one day you think oh why don't we try anthropic instead but there's a problem anthropic API is a little bit different to open AIS this is especially true for things like streaming structured outputs tool calling so you need to build all of this extra glue code just in order to try out a different model the AISD solves this problem it's a library you can call which handles the interaction between you and your LM so now you can call the AISD K instead and it will handle all of the glue code not only that but it has some really nice helpers for common use cases it helps you stream text do structured outputs tool calling and even handle agents I think of it like the typescript standard library for working with AI the AI SDK is maintained by versel but you don't need to use versel in order to use it you don't need to pay Vell any money it's free open-source software there are three different parts of the asdk the first is the core this is used for your backend code so node Bund Dino whatever you also have the asdk UI which is a set of framework agnostic Hooks and components this is for linking up a front end to an aisk backend there's also the AISD RSC which is their react server components offering this is for doing things like generative UI all that stuff this tutorial is just going to focus on the core it's going to fill you in on the basics and give you the grounding to build virtually anything it has maybe the best npm package name of all time which is npm install AI all of the core stuff is under the AI package and you can integrate with different providers by installing their custom package for instance open AI is a AIS SDK open Ai and then you can choose your model by calling open Ai and then passing in the model that you want so that is the lay of the land you get a unified API where you can drop models in and out you can stream text generate text work with structured outputs tool calls and then build agentic loops and we're going to cover all of that in this tutorial let's start with the absolute Basics generate text let's look at literally the simplest setup that the AISD supports using an llm to generate text we're first going to grab our model from anthropic in our case we're going to use the latest version of haiku now we're going to create a function called answer my question which is going to take in a prompt next we're going to grab the function generate text from the AI package I'm in the AI hero repo which I have already downloaded asdk anthropic and the AI package so I'm good to go inside our function which you'll notice is asynchronous we're going to await the result of generate text passing our model and the prompt this is is going to return us an object that contains a text property which is a string we're then going to return text from our function let's ask it a question we're going to ask it what is the chemical formula for dihydrogen monoxide when we run this it's going to call anthropic with our query and return the generated text logging it to the console the chemical formula for dihydrogen monoxide is H2O lovely I've got an anthropic API key in my environment variables which is being picked up by the script I'm using to run this you can find all the instructions for setting that up inside the AI hero repo it's not only text that we get back from generate text there's also a bunch of other stuff in here which we'll cover in our later examples but that is the simplest possible demo for the AISD K we declared our model in this case anthropics Haiku we grabbed generate text from AI we called it using the model and the prompt we console loged the text and so we were able to ask a question and get back an answer very very cool that is but a mere glimpse of the power that is at our disposal in our previous example we saw how you can generate text using the aisk but this spits out all the text all at once right at the end what if we need to stream the text in token by token for that we can use the stream text function from the aisk this takes in a model and a prompt in exactly the same way but instead of returning text it returns a text stream the text stream is an asyn itable that means it can be streamed to a file or over a network connection in this example we're just going to stream it to standard out this foral weight Loop waits for every chunk of the text stream and then puts it directly into standard out let's say we ask it what is the color of the sun if we run this code we're now going to see it streaming to our console and from here it's pretty easy to imagine just hooking this up to a network request and just streaming it to a UI the stream text function also returns a text promise this means if you just want access to the final text you can just await text so this is a nice option to have if you want to have access to both the stream and the final generated text sometimes you need the AI to act in a certain way no matter which prompt it receives in this example we're building a text summarizer and so we want the AI to summarize the text that it receives we want to give it a role we want to give it instructions and we want to do all of that before it receives the prompt from the user to do that we can use a system prompt doing that in vel's a dek is as simple as passing a system property under the hood this preens a special message with the role of system that has our system prompt in it we could do that in the AISD as well if we want to we can get rid of both prompt and system and instead pass a messages array so this is what a system prompt looks like under the hood this can be done in generate text stream text and all of the other apis that we're going to look at throughout these examples using system prompts is one of the key tools for working with llms so it's really nice that the asdk makes it that easy I wanted to demonstrate for you just how flexible the AIS is when it comes to model selection here we have an Ask function that takes in a prompt and a model of type language model we can call this function with any model that the aisk provides let's say we create a prompt here then we grab anthropic from AISD anthropic now we can pass that into ask by just having the prompt passed in and then anthropic calling it with the model we want to select if we want to call it with a different model well we can just call it with an open AI model which we can get from the asdk open aai package this just gives you a ton of flexibility with how you want to build your application the ask function is totally decoupled from the model that it uses so this type of language model that's exposed by the aisk basically lets you do dependency injection you can inject any model into any system and it will just work to me this flexibility is one of the core selling points of the AIS SDK it's pretty normal if you're building any kind of chat bot to want to keep track of the conversation history this is so the llm has context over the conversation you've already had so you can ask follow-up questions without having to rephrase your question every time not only that but understanding how conversations are persisted is really important for communicating with an LM over the wire and we're going to show how to do that with the Vel AISD K in this example let's start by understanding what the shape of a conversation history might look like the asdk exposes a type called core message this is an object that represents a message in a conversation and if we understand this one type I promise we'll understand everything about conversation history so we've created an array of messages saying that this is an array of core message core here just refers to the fact that we're in the AISD core instead of aisk UI or RSC every message has to contain a role property this is a string that can be assistant system tool or user let's say that it's user for now we also need to add the content of the message for instance the content of this message is the user saying hello you to represent the llm replying we can use the assistant role so again in this conversation we have the user saying hello you and the assistant the llm saying hi there we had a brief look at system prompts earlier they're represented in the message history via a role of system in this case we're telling the llm that you are a friendly greeter greet everyone who comes by and the final role of tools here we'll investigate that when we look at tool calling later as the conversation gets longer and longer more user and assistant messages are going to be appended to these messages and this simple array with these different roles and content Parts is basically all you need to represent a conversation now we understand about what the messages array is let's apply it to a real worldish situation for the first time here we're actually going to spin up a server to give us a kind of server boundary to work around we're going to put our server inside a start server function we're going to grab the hono package and then create a new app inside our start server function We'll add a root to our server then we're going to grab serve from hono node server and then serve it inside our start server function we need to wait until the server is listening before we can make requests to it for that we're going to need to listen for the listening event this is a great tip here you can actually grab once from node events and then inside start server just after the server's been declared we can await once server listening so declaratively this will just wait for the listening event to fire very nice finally we're going to return server from start server next let's build the API call our API get completions route is going to accept an array of call messages from AI since this is just a demo I'm not doing any actual validation here it's just going to be assumed that this is what you're going to pass then we're going to take those messages which is the message history and we're going to pass them into generate text from AI as well as a model that I've got earlier this means that based on the conversation history that it's going to receive from messages the llm will work out what to say next the messages that it suggests will be added to result. response. messages so from our API endpoint we can return those as Json using CTX dojon to some of them we've created a little server endpoint that receives a bunch of messages and then Returns what the next message should be this is a pretty common pattern it means our server is completely stateless the client is responsible for tracking the messages and working out which messages it should send next there are other ways of doing this like your client could send just the new message and the server could maintain the state of the message history but the way we've got it working right now where the client keeps control of the messages is kind of fine for a now that we've created our server let's actually call it from the client we're going to create a new array of messages to send asking what's the capital of Wales we're also going to start our server here importing it from server. TS we're then going to fetch from our server we're fetching from Local Host 4317 with an API get completions with a method of post passing the messages to send as a body Json stringified then with the result coming back we pass it into Jason and we know that it's coming back as a an array of call messages again since these are only the new messages let's concatenate them with all of the messages so the previous messages we were sending and then log them out and when we run this we get an output that looks like this our original question here has been appended to with a role of assistant answering the question and this messages array is ready to receive more messages to keep on adding to the conversation history in this example we've seen how to keep track of a conversation history using the Vel aisk we've learned about the messages array including all the various roles and we've also seen a pattern where you pass all of the messages to a server that can then append the newest message to the end well done for sticking with me on that one that was quite some Journey you can use vel's asdk to connect to locally running models in fact not just locally running ones but ones hosted at any URL the AISD has a function called create open AI compatible this lets you communicate with models that have an open aai compatible API in our case I'm using an app called LM Studio which exposes this API on Local Host 1 1234 so I can install AISD open AI compatible and create this LM Studio provider I can then use this provider to grab a model I'm using an empty string here because if you pass an empty string it will default to the model you have selected in LM Studio I can then use this model by passing it into generate text so it works just like the open AI anthropic models we've seen I've specified Max retries as zero here by default the SDK will retry queries multiple times I think three by default default just to make things more robust but since the model is on our local network we want it to fail instantly if it can't get through let's give this a go we're going to ask the llm a story about its grandmother and if we run this we get this quite detailed story about Nana's love for cooking good old llms so this is a nice simple setup for how you can connect the Vel aisk to a local model often the thing that you want to get back from your llms is not text but some kind of object a classic example is data extraction where you want to to extract multiple properties from a single input let's say you're scanning a bank balance for the account number and the current balance the most efficient way to do this is with structured outputs this lets you ask the llm a question give it a format and it will reply in that format in this example we're going to ask the llm for a recipe we want the name of the recipe the ingredients for the recipe which are objects inside an array and then the steps to create this recipe the first step is to create a Zod schema which represents the object that we want to get back you can see that we've got name ingredients and steps here all declared in their Zod equivalents if you've never seen Zod before then I have a free tutorial on my sister site total typescript which are linked to below we can then take this schema and pass it directly into a function called generate object from AI this is receiving our model The Prompt and the schema let's also add a simple system prompt to create recipe just to give the LM a bit more context the result that comes back from generate object has a property called object which contains our recipe and thanks to typescript being clever we actually get all of the properties autoc completed for us not only does Zod describe what object we want to the llm it also guarantees that the object is that shape when it comes back so we can be pretty sure that recipe. name will definitely be defined which is very very useful but we're not quite done here we should provide the llm with more information as to what each property actually means currently all it's got to go on are name ingredients and steps here we can do this by adding zods do describe function to each of these properties now it's clear to the AI what we're asking for when we're talking about each property this is especially useful when the names are a little bit ambiguous or not that descriptive and finally we can pass a schema name to the generate object function itself we could also provide schema description here but I think in this case it feels a bit like Overkill so let's give this to go and see what outputs we get let's ask it how to create babagan and when we run this we're going to get back a recipe for babaan so there we go that's how we get structured outputs from vel's aisk we use generate objects instead of generate text we pass it a schema with a Zod schema and we use schema name descriptions and the dot describe function to describe what we're doing to the llm structured outputs are incredibly useful and it's great the AI Decay makes it so easy in our previous example I showed you how to get structured outputs from an llm but the outputs were all generated at once we waited for a little bit we twiddled our thumbs and then we just sort all the outputs all at once this is because we were using generate object here but what if you want to see the object as it's being generated what if you want to stream the object well you can do that by changing generate object to stream object here you'll notice a couple of changes here from the previous example first of all because we're now using stream we have to wait for the final result so result. object has now become a promise that we await for the final object the reason for that is that stream object Returns the first chunk of the object as soon as the chunks come in so the result doesn't have the final object yet so we got to await it if we want access to the object as it's being generated we can use result. partial object stream this is an asyn kable meaning we can wait for each chunk to come in with a for a we Loop and then log it to the console we're going to clear the console first and then console. dear it so it should see it streaming in live let's give this a crack and see what updates we get we're going to ask it how to make when we run this the objects are being generated as we go wo did you see that that was fast but you notice there was progress as we went through let's try this one more time and there it goes beautiful the final chunk then of this stream the result. partial object stream is the entire completed object so the way you'd use this in an application is instead of just logging it to the console you would send these chunks over the network that way your users could see the object being built up in real time it's always nicer to see a progress indicator rather than just wait in and having it spat out at the end so that's what the stream object function does in the aisk it's a really nice userfriendly API for building up objects over time another classic use case for llms is classification that is classifying things into different categories let's say we want to take a user's comment here pass it to the llm and then get back one of three strings positive negative or neutral in traditional software parin this is called an enum set of enumerated values to do this we're not only going to pass the user comments but also describe the structured outputs we want back from the llm and to do that we're going to use the aisk we're going to start with a classify sentiment function and inside it we're going to call generate object from the Vel AI SDK the difference here is that we're passing an output of enum we can also then pass an enum property describing the enumerated members of this enum the result we get back from generate object contains an object property as we've seen before and this object confusingly is our enum we can see that it has the type of positive negative or neutral let's try out a few different statements to see how it works we can start with I'm not sure how I feel and if we run this I'm not sure how I feel comes out as neutral let's try this is terrible and as expected this comes out as negative and just like that we've got a pretty simple sentiment analysis system we use generate object from the AISD passing it an output of enum passed it the enum and then just ran some text through it this is a really really great use case for enums and it's great that the aisk makes it so simple this simple primitive can be used for all sorts of classification tasks another thing that the AIS SDK handles out of the box is passing images and files to llms you can take this file pass it to the llm then the llm can return text or images or call tools with it it's an extremely powerful feature not all LM support it but for the ones that do the vasel AIS SDK lets you handle it in our case we want to pass an image to the llm and make it generate a description that we're going to use as out text for that image alt text can be used on a website or an app to describe an image for folks that can't actually see it we're going to start by using a pretty simple system prompt saying we don't want to pass 160 characters use Simple language be concise and create an out text for the image we're then going to create a function called describe image which is going to receive an image path on our local file system this looks fine but we're not actually using the image path yet inside our function we can't use the image path directly as a problem we first need to load the image into memory and then pass that binary into generate text to load it into memory we're going to grab read file sync from node.js this stores the image in memory as a u inate array which is basically a raw binary representation of the image for the node nerds out there yes it's technically a buffer but a buffer is just an uate array with a few extra methods we're using the sync version of the API here but of course we could use read file from FS promises if we wanted to if we were really concerned about non-blocking IO but for this read file sync is perfectly fine now we've just got to find a way to pass this image as uate array into generate text we can't pass it in as a prompt directly so we have to pass it in using the messages array this is in the same format that we saw before when we looked at chat history but here we now have a Content array where we're passing a type of image and the image is the images uate array let's try running this I've got an image of some fireworks that I would like to test and so I'm going to call describe image using path. jooin to grab the local fireworks JPEG and when we run this we're going to get back a description of the image that we just saw colorful fireworks display over a city skyline at night Spectators watch from the shoreline it's like Tolstoy to sum up then we took an image put it into memory using read file sync passed that in the messages array as a uate array and got back a description using generate text now our current approach which works if you have the file in memory here but what if you only have a URL to the file well for that there's a really nice shortcut we can use let's first change this to image URL instead we no longer need to read file syncit since we're not going to load it in memory and then instead of passing images uate array we're going to pass in new URL image URL we're wrapping the image URL in new URL to indicate to the aisk that this is a URL we want to pass directly to the llm I've got an image here which which I'm hosting on GitHub so it's available at a public URL I'm not sure where this is but it looks very pretty I would love to go there and now instead of pointing at a local file we're going to point at this URL let's see what happens when we run this it's going to go to that URL try to pull down that image and then describe it for us Lake bled in Slovenia calm water reflect buildings and Autumn trees so there we go new URL is a really nice shortcut you can take when you want to point to a URL that is public available so just like that we've built an ALT text generator using the Vel aisk passing it through the messages array and either passing a file in memory or locating a URL and passing it that URL extracting structured data out of unstructured data is one of the most powerful use cases for llms in this example we're going to be passing a PDF to an llm and getting it to analyze it for us in this case it's going to be a PDF of an invoice and we're going to extract all sorts of information like the currency total amount invoice number from that invoice this structured data is going to be so much more useful than this unstructured PDF here means we can store these fields in a database query them filter them all that stuff by the way this is the demo document that we're going to be analyzing it has a dollar amount here it's obvious it's from Australia because it's Australia fresh produce it's got some addresses here and it's got a detailed list of everything on the invoice since we need structured data to come back from the llm we're going to to create a massive Zod schema we've got the total the currency the invoice number the company address company name and invoice address all with these describe functions on them to basically say what these fields are I know I've talked about these describe Fields a lot but oh my God they're so important this is prompt engineering you are telling the llm what these fields do what it's supposed to be looking for in the document let's create an extract data from invoice function inside we're going to call generate object here passing in our model which is going to be clawed then we're going to use a system prompt very simple one and then passing in the schema we're expecting an invoice path which is going to be the path on our local file system to the PDF in the example that we saw before we passed messages into here and we passed an image into this content array we can do the same thing with the PDF we're still reading the invoice path turning it into a uate array and passing that into Data now instead of type of image we now have a type of file instead we also need to pass in a mime type in here to tell the llm what kind of file it's receiving it could probably work this out on its own by just checking the magic numbers of the file but you know it's polite isn't it finally we're going to go back to generate object up here and return the object from it this object just like before matches the schema shape that we described above let's give this a go I've got a path to the invoice and when we run this we get back the data from the invoice we can see that the totals match 39.6 39.6 the company address 1 23 somewhere Street in Melbourne and even the company name of Sunny farm and just like that we've got a reusable function where we can extract the required data from any invoice we use generate object passing it a schema passing the file as a u dat array to the messages array passing a mime type so it knew exactly what sort of file it was dealing with then we use structured outputs with a really really detailed schema to get back structured data from unstructured inputs it is wild how simple this is given how difficult a problem this has been historically but oh my God you you can do so much cool stuff with this anyway that is how you pass files to llms using vel's aisk vel's AISD has a really simple way to create embeddings I'm going to assume that you know what embeddings are for the purposes of this video but essentially they're a way to represent words images or data in high dimensional space you can then compare them based on these Dimensions to check how similar two things are to each other this is called a semantic similarity comparison this makes embeddings a really powerful primitive for things like search and categor ization in this exercise we're going to be embedding some words in a very simple Vector database the first step is to grab the embedding model the one we're using is from LM Studio this is running on my local machine if we wanted to use open aai instead we would import it from open aai and call open ai. embedding in this simple example we're just going to embed some words dog cat Car and Bike to create the embeddings we're going to grab the embed many function from Ai and then create the embeddings passing in our local model and the values let's try running this to see what we've created here we can see here that we've created an array of vectors now each Vector there are four vectors one for each word each one is an array of numbers so we've got I think hundreds and hundreds of numbers yeah 668 more items in each Vector each one of these numbers represents a dimension in multi-dimensional space and so for each Vector there are four vectors one for each word we have over 668 different dimensions and so thanks to the embed many function we now have vectors for dog cat C and bike inside embeddings we're now going to collect these together into a very simple Vector database we're mapping over the embeddings here and then basically storing the embedding next to the value that it represents so Vector database ends up being just an array with the value which is the string and then the embedding which is the vector that we saw in the real world you would store these in an actual database using something like postgress and PG Vector but for small numbers of values this is perfectly fine too let's now use our Vector database we're going to use it to search for the most similar word to a search term that we choose to do that we're going to have to embed one more word so we'll grab embed from Ai and now we're going to embed our search term so we're going to search for the most similar word to K9 if we look at search term here then we've got search term. embedding which is the same as all the vectors that we've got already so now amazingly we have all the information we need all we need to do now is a bit of maths or sorry math for my American listeners we just need to go into our Vector database find the embedding which is most similar to our search term. embedding and then return that to compare these two vectors the AISD K exposes a cosine similarity function so we can grab that from AI here and now we're going to map over each member of our Vector database and do something interesting we're first just going to return the value and then we're going to return how similar it is using cosine similarity this takes in two parameters the member of our Vector database and the search term embedding and then finally we're just going to sort the these by similarity here using do sort this should sort the most similar one to the top when we run this we can see the results here coine similarity returns a number between zero and one and so dog is the most semantically similar word to Canine and it turns out bike is the least similar interesting if we change our search term here to feline then we should see that cat is the most similar to feline how about something like pedal well none of these are very similar to Pedal but bike is the most similar car is the next most similar this relatively simple setup here can drive all sorts of awesome features like search and categorization we grabbed our list of desired values here we then created an embedding for each one we loaded those into a vector database we embedded a vector for our search term in this case pedal and then we mapped over the vector database using coine similarity to compare the vector for the search term for the one in our database this is so useful because you can just create the embedding when the entity is created and then when you search for it all you need to do is just embed your search term this makes it a pretty efficient operation and it's really nice that the aisk exposes the embed embed many and cosine similarity functions so far we've been getting llms to answer questions scan documents and do data extraction but they can do a lot more than that llms can interact with the world the way they do that is alongside the prompt giving them a set of tools they can call and then the llm describes which tools it would like to be called using tool calls the results of those calls end up as tool results which we can later if we want to pipe back into the llm to feed it more information but let's not get ahead of ourselves for now all we're going to do is build the simplest tool calling llm imaginable we're going to create the most expensive and pointless console log of all time to get started I'm going to grab my tool calling model in this case it's Haiku from anthropic and then going to grab the tool function from Ai and our tool is going to be called log to console tool the first thing that any tool needs is the description of parameters that it's going to receive this is done with a Zod schema which in our case looks like this we should also use describe really to make sure that the LM understands what each parameter does next we need to say what the tool is going to do we do that by specifying an execute function the execute function can be a sync so it can virtually do anything it can write to a database it can call apis but in our case it's just going to log to the console and of course this is really nicely typed safe too if we rename this to Mage then it's going to be mage Mage instead next we're going to add a description to the tool this tells the LM again what it's supposed to do with the tool this is all prompt engineering now that our Tool's been created let's actually use it inside a generate text call we're going to create a log to console function which receives a prompt and then pass that to generate text along with the model we're going to give it a system prompt to rather dramatically your only role in life is to log messages to the console tell it to log messages to the console and finally we can pass a record of different tools here and we're going to say log to console is the log to console tool so to recap we have created a tool we have passed it to generate text and given it a simple system prompt to encourage it to use the tool let's try this out with the traditional greeting and when we run it we're going to see Hello World being printed out to the console now that's pretty good but it's quite opaque What's Happening Here how do we go in and debug this we're going to grab the steps property out of generate text here steps is an array of each of the steps taken by the process we can look into each step see what was passed to the llm and see what tool calls and Tool results were generated for now let's just go into steps grab the first step and then see what tool calls were called on it when we run this we can see that the tool name log to console was called we can also see the arguments it was passed args message hello world if we were to try changing this to Tool results instead then we can see the result of our tool call in this case we didn't return anything so the result is undefined these tool results can be fed back into the llm to make it more powerful provide it more information especially when run over multiple steps so we've ended up with something pretty powerful here in very little code we created a tool grabbing the tool from AI we gave it a description a Zod object of parameters and a function to execute note by the way the llm itself isn't executing this it's giving us the tool calls that it wants to happen and we execute it that's why console log is happening on our console not anthropics console finally we pass all of that to generate text along with the system prompt and it logs to the console calling our tool we can debug that by looking in steps and either looking at tool calls or tool results so there we go that's a really simple introduction to Tool calls which are incredibly powerful primitive in our previous example we looked at tool calling where you can pass the prompt as well as the tools to the llm and it will return you a set of calls that it wants you to make and this example we're going to look at what happens when you pipe the results of those tools back to the llm this can create a powerful feedback loop where the llm is continually grounding itself in the real world asking questions with its tools and receiving answers from Tool results and this feedback loop is what most people including anthropic call agents the Vel aisk makes this concept of a loop super simple with something called steps and we're going to make an agent that can retrieve the weather for us at a city we specify to kick that off we're going to use our small tool calling model again hi coup then we're going to create our get weather tool we give it a description as well as some parameters now we're going to implement the execute function in this case we're just going to stub It Out by always returning the same thing the weather in the city that you specify is 25° and sunny but because it's a sync if we wanted to we could go away and actually call a weather API to get the actual weather for that City next we're going to hook this tool up to a function called ask a question we're going to call stream text with a model and a prompt and the tools and then like we've seen before with stream text we're going to write the text stream to standard out as it streams in and finally we're going to ask it what the weather's like in London now when we run this we notice something interesting we don't actually get the information that we're looking for it just says I'm going to help you check the weather why would this be happening let's debug this with the same strategy we used before grabbing steps from stream text instead of logging out the text stream instead we're going to log steps and because we're using stream text we actually have to await the result of steps steps is a promise here and here's what it spits out there are several things to notice here the first is that it only took one step we can see that it called one tool here get weather and it returned the result of the weather in London is 25° and sunny so it seemed like if we were just able to call the llm again with the information in the tool like take another step it would be okay and in other words it seems like the llm got the result of the tool but then it didn't feed it back to itself so the question is how do we create this Loop here how do we make the llm take more than one step the way to change this is to pass Max steps to stream text and we're going to specify as two this means that instead of stopping this loop after one iteration it's actually going to go two iterations round so the llm will choose the tool calls it will get the results back in the llm and then it will decide what to do again I'm going to revert this code to what it was doing before so that we can see the stream and when we run this now we get a really cool output we can see the first step here and then it reacts to that information and even says that it sounds like a great day to be outside let's try something interesting now what would happen if we set the max steps to 10 turns out that we get nearly the same result the llm actually stops itself after two steps I've done some debugging into the steps here and on the second step we have a finish reason of stop this means that the llm itself actually decided no I have completed my task it is time to stop so max steps really operates as a safety mechanism you can specify the lowest number really but if you do specify something above it then the llm May itself decide to stop but it's really not a good idea to specify something like Infinity because you know that might cost you a bundle so to wrap up we've seen how with one simple property Max steps you can create an agentic Loop where the result of your tool gets fed back into the llm this lets you create a gorgeous feedback loop where the llm is continually grounding itself and if you do this right it makes hallucinations a lot less likely and makes the llm a lot more useful and that folks is my vsel aisk tutorial if you are hungry for more there is only one place I recommend and that is AI hero we're going to cover all sorts of stuff from evales to prompt engineering to rag to agents in the future I'll be diving into developer productivity too creating your own coding agents and getting the most out of existing tools like cursor but mostly I'm just really thankful to you thank you for getting to the end of my longest YouTube video ever this really has been a ton of work to put together and if you've enjoyed it then please leave a like and hit subscribe and if you want more of this stuff then go hit up my newsletter on AI hero thank you so much for joining along and I will see you very soon
